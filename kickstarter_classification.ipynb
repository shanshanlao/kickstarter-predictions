{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading libraries\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import the dataset\n","kickstarter = pd.read_excel('Kickstarter.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For the purpose of this project, we only need to include projects with \"successful\" or \"failure\" state\n","df = kickstarter[kickstarter['state'].isin(['successful','failed'])]\n","\n","# Convert the target variable to binary\n","df['state'] = df['state'].replace(['successful','failed'],[1,0])\n","\n","# Convert goal to usd_goal = goal * static_usd_rate\n","df['usd_goal'] = df['goal'] * df['static_usd_rate']\n","df = df.drop(columns = 'goal')\n","\n","df.shape"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check the number of rows and columns \n","df.shape"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Detect Duplicated Records"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check if there is any duplicated records\n","df = df.drop_duplicates()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.2 Drop out-of-scope predictors"]},{"cell_type":"markdown","metadata":{},"source":["According to the project instruction, we can only use the predictors \"that are available at the moment when a new project is launched.\"\n","Therefore, we do not need any predictors regarding 'states' of the project."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df.drop(columns = ['state_changed_at','state_changed_at_weekday','state_changed_at_month', 'state_changed_at_day', 'state_changed_at_yr','state_changed_at_hr','launch_to_state_change_days'])"]},{"cell_type":"markdown","metadata":{},"source":["The information about pledged, staff_pick, backers_count, spotlight wont be available at the moment when the project is launched. Therefore, we remove them as well."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df.drop(columns = ['pledged','usd_pledged','staff_pick','backers_count','spotlight'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.3 Identify unique identifiers"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Look at the number of unique values in each column"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.nunique()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Note that disable_communication only have one unique value, so it won't be useful for our prediction.\n","\n","[id, name, deadline, created_at, launched_at] are almost a unique identifier, so we should drop them as well."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df= df.drop(columns = ['disable_communication','id','name','deadline', 'created_at', 'launched_at'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.4 Handle with Missing Values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check if there is any missing values\n","missing_values = np.where(pd.isnull(df))\n","\n","# Identify the columns that contain missing values\n","df.columns[list(set(np.where(pd.isnull(df))[1]))]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Count the percentage of missing values\n","len(missing_values[0]) / df.shape[0]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["10% of missing values is not that small. Let's try look into the column."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['category'].unique()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Instead of dropping the missing values, we can replace the null value with 'Unknown'."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['category'] = df['category'].fillna('Unknown')\n","df.shape"]},{"cell_type":"markdown","metadata":{},"source":["### 1.5 Detect collinearity between variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check if there is any collinearity between variables\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(26, 6))\n","heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n","heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);\n","plt.savefig('heatmap_classification.png', dpi=300, bbox_inches='tight')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["According to the correlation heatmap, following varaibles have high correlation:\n","- name_len and name_len_clean, \n","- blurb_len and blurb_len_clean,\n","- deadline_yr and created_at_yr and launched_at_yr\n","\n","For each pair, we only need to keep one of them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df.drop(columns = ['name_len_clean','blurb_len_clean','created_at_yr', 'launched_at_yr'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['state'].value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.6 Handle Categorical Variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check variable types\n","df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For 'weekday' variables. convert them into numerical variable from 1-7\n","cols = ['deadline_weekday','created_at_weekday','launched_at_weekday']\n","df[cols] = df[cols].replace(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],[1,2,3,4,5,6,7])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['category'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['country'] = np.where(df['country'] == 'US',1,0)\n","df['currency'] = np.where(df['currency'] == 'USD',1,0)\n","\n","# Then country and currency will become idential, drop one\n","df = df.drop(columns=['currency'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.get_dummies(df, columns = ['country','category'])\n","df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.7 Remove Outliers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import IsolationForest\n","iforest = IsolationForest(n_estimators=100,contamination=.05,random_state=0)\n","\n","newdf = df[['usd_goal','static_usd_rate','name_len','blurb_len','create_to_launch_days','launch_to_deadline_days']]\n","pred = iforest.fit_predict(newdf)\n","score = iforest.decision_function(newdf)\n","\n","# Extracting anomalies\n","from numpy import where\n","anomaly_index = where(pred==-1)\n","anomaly_values = df.iloc[anomaly_index]\n","\n","anomaly_values\n","for idx in anomaly_index:\n","    df = df.drop(idx, errors='ignore')\n","\n","df.shape"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df.loc[:,df.columns != 'state']\n","y = df['state']\n","\n","# Standardize the predictors\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_std = scaler.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feature selection using Random Forest\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.inspection import permutation_importance\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","rf = RandomForestClassifier(random_state=0)\n","model = rf.fit(X_std, y)\n","\n","result = permutation_importance(rf, X_std, y, n_repeats=1,\n","                                random_state=0)\n","perm_sorted_idx = result.importances_mean.argsort()\n","\n","tree_importance_sorted_idx = np.argsort(rf.feature_importances_)\n","tree_indices = np.arange(0, len(rf.feature_importances_)) + 0.5\n","\n","fig, (ax1) = plt.subplots(1, 1, figsize=(6, 10))\n","ax1.barh(tree_indices,\n","         rf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n","ax1.set_yticklabels(X.columns[tree_importance_sorted_idx])\n","ax1.set_yticks(tree_indices)\n","ax1.set_ylim((0, len(rf.feature_importances_)))\n","\n","fig.tight_layout()\n","plt.show()\n","plt.savefig('RandomForest_FeatureImportance.png', dpi=300, bbox_inches='tight')\n","\n","# Print feature importance\n","#pd.Series(model.feature_importances_, index = X.columns).sort_values(ascending = False).plot(kind = 'bar',figsize = (14,6))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df.loc[:,df.columns != 'state']\n","sorted_features = pd.Series(model.feature_importances_, index = X.columns).sort_values(ascending = False)\n","chosen_features = sorted_features[:15].index.to_list()\n","#chosen_features = sorted_features[sorted_features > 0.04].index.to_list()\n"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Classification Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df[chosen_features]\n","\n","#X = df.loc[:,df.columns != 'state']\n","y = df[\"state\"]\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_std = scaler.fit_transform(X)\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.33, random_state = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","lr = LogisticRegression(max_iter = 1000)\n","\n","from sklearn.tree import DecisionTreeClassifier\n","dt = DecisionTreeClassifier(max_depth=10)\n","\n","from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(oob_score=True)\n","\n","from sklearn.ensemble import GradientBoostingClassifier\n","gbt = GradientBoostingClassifier(random_state = 0)\n","\n","from sklearn.neural_network import MLPClassifier\n","ann = MLPClassifier(hidden_layer_sizes=(3), random_state=0)\n","\n","from sklearn.svm import SVC\n","svm = SVC(kernel=\"linear\", random_state=0) #, C=0.5, gamma = 3)\n","\n","from sklearn.model_selection import cross_val_score\n","scores_log = cross_val_score(lr, X=X_std, y=y, cv=5)\n","scores_rf = cross_val_score(rf, X=X_std, y=y, cv=5)\n","scores_gbt = cross_val_score(gbt, X=X_std, y=y, cv=5)\n","scores_ann = cross_val_score(ann, X=X_std, y=y, cv=5)\n","scores_svm = cross_val_score(svm, X=X_std, y=y, cv=5)\n","\n","print(scores_log, scores_rf,scores_gbt,scores_ann, scores_svm, sep='\\n')"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","lr = LogisticRegression(max_iter = 1000)\n","model_logit = lr.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 DecisionTree"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","dt = DecisionTreeClassifier(max_depth=10) # default is to grow a full tree\n","                                    # avoid overfitting\n","model_dt = dt.fit(X_train,y_train)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3 Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Build the model\n","from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(oob_score=True)\n","model_rf = rf.fit(X_train, y_train)\n","\n","model_rf.oob_score_"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4 Gradient Boosting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","gbt = GradientBoostingClassifier(random_state = 0)\n","model_gbt = gbt.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.5 K-Nearest Neighbors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","accuracy = 0\n","bestK = 0\n","for i in range (1,21):\n","    knn = KNeighborsClassifier(n_neighbors=i)\n","    model = knn.fit(X_train,y_train)\n","    y_test_pred = model.predict(X_test)\n","    if accuracy_score(y_test, y_test_pred) > accuracy:\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        bestK = i\n","    #print(\"k = \", i, \"accuracy =\", accuracy_score(y_test, y_test_pred))\n","\n","knn = KNeighborsClassifier(n_neighbors=bestK) #,  weights = 'distance')\n","model_knn = knn.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.6 Artificial Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find the optimal size of hidden layer\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import cross_val_score\n","for i in range(1,21): \n","    model = MLPClassifier(hidden_layer_sizes=(i), max_iter = 1000, random_state=0)\n","    scores = cross_val_score(model, X=X_std, y=y, cv=10)\n","    print(i, \":\", np.average(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier\n","ann = MLPClassifier(hidden_layer_sizes=(3), random_state=0)\n","model_mlp = ann.fit(X_train,y_train)\n","\n","model_metrics(model_mlp,X_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Find the best hyper-parameter\n","from sklearn.model_selection import GridSearchCV\n","mlp = MLPClassifier(max_iter=5000, random_state=0)\n","\n","parameter = {'hidden_layer_sizes': range(1,22)}\n","grid_search = GridSearchCV(estimator = mlp, param_grid= parameter, \n","                           scoring = \"accuracy\", verbose=True)\n","model_mlp = grid_search.fit(X_std, y)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.7 Support Vector Machine"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find the optimal gamma\n","from sklearn.svm import SVC\n","for i in range(1,11):\n","    svm_rbf = SVC(kernel = \"rbf\", random_state=0, C =0.5, gamma = i)\n","    model_rbf = svm_rbf.fit(X_train,y_train)\n","    scores = cross_val_score(model_rbf,X=X_test, y=y_test, cv=10)\n","    print(\"gamma = \",i,\", score = \", sum(scores)/len(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Build the SVM model using a linear model\n","from sklearn.svm import SVC\n","svm = SVC(kernel=\"linear\", random_state=0, C=0.5, gamma = 3)\n","model_svm = svm.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["### Model Performance Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import metrics\n","\n","def model_metrics(model,X,y):\n","    y_pred = model.predict(X)\n","\n","    accuracy = metrics.accuracy_score(y, y_pred)\n","    precision = metrics.precision_score(y, y_pred)\n","    recall = metrics.recall_score(y, y_pred)\n","    f1_score = metrics.f1_score(y, y_pred)\n","\n","    model_metrics = [accuracy, precision, recall, f1_score]\n","    return model_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_performance = {\n","    'Logitstic': model_metrics(model_logit,X_test,y_test),\n","    'KNN': model_metrics(model_knn,X_test,y_test),\n","\n","    'DecisionTree': model_metrics(model_dt,X_test,y_test),\n","    'RandomForest': model_metrics(model_rf,X_test,y_test),\n","    'GradientBoosting': model_metrics(model_gbt,X_test,y_test),\n","\n","    'ANN': model_metrics(model_mlp,X_test,y_test),\n","    'SVM': model_metrics(model_svm,X_test,y_test)    \n","}\n","\n","pd.DataFrame.from_dict(model_performance, orient='index',columns = ['accuracy','percision','recall','f1_score'])"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Evaluation on Grading Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading libraries\n","import pandas as pd\n","import numpy as np\n","\n","# Import the dataset\n","grading = pd.read_excel('Kickstarter-Grading-Sample.xlsx')\n","\n","# For the purpose of this project, we only need to include projects with \"successful\" or \"failure\" state\n","df_test = grading[grading['state'].isin(['successful','failed'])]\n","df_test['state'] = df_test['state'].replace(['successful','failed'],[1,0])\n","\n","# Convert goal to usd_goal = goal * static_usd_rate\n","df_test['usd_goal'] = df_test['goal'] * df_test['static_usd_rate']\n","df_test = df_test.drop(columns = 'goal')\n","\n","# Check if there is any duplicated records\n","df_test = df_test.drop_duplicates()\n","\n","### Drop out-of-scope predictors\n","df_test = df_test.drop(columns = ['state_changed_at','state_changed_at_weekday','state_changed_at_month', \n","'state_changed_at_day', 'state_changed_at_yr','state_changed_at_hr','launch_to_state_change_days',\n","'pledged','staff_pick','backers_count','spotlight','disable_communication',\n","'id','name','deadline', 'created_at', 'launched_at','usd_pledged','name_len_clean','created_at_yr', 'launched_at_yr'])\n","\n","### Handle Categorical Variables\n","cols = ['deadline_weekday','created_at_weekday','launched_at_weekday']\n","df_test[cols] = df_test[cols].replace(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],[1,2,3,4,5,6,7])\n","\n","# dummify the other categorical variables\n","df_test['category'] = df_test['category'].fillna('Unknown')\n","\n","df_test['country'] = np.where(df_test['country'] == 'US',1,0)\n","df_test['currency'] = np.where(df_test['currency'] == 'USD',1,0)\n","df_test = df_test.drop(columns=['currency'])\n","df_test = pd.get_dummies(df_test, columns = ['country','category'])\n","\n","#df_test = pd.get_dummies(df_test, columns = ['country','currency','category'])\n","\n","# Testing\n","X_grade = df_test[chosen_features]\n","#.loc[:,df.columns != 'state']\n","y_grade = df_test[\"state\"]\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_grade = scaler.fit_transform(X_grade)\n","\n","\n","### PCA\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=3)\n","pca.fit(X_std)\n","# X_grade = pca.transform(X_grade)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_performance = {\n","    'Logitstic': model_metrics(model_logit,X_grade,y_grade),\n","    'KNN': model_metrics(model_knn,X_grade,y_grade),\n","\n","    'DecisionTree': model_metrics(model_dt,X_grade,y_grade),\n","    'RandomForest': model_metrics(model_rf,X_grade,y_grade),\n","    'GradientBoosting': model_metrics(model_gbt,X_grade,y_grade),\n","\n","    'ANN': model_metrics(model_mlp,X_grade,y_grade),\n","    'SVM': model_metrics(model_svm,X_grade,y_grade)\n","}\n","\n","pd.DataFrame.from_dict(test_performance, orient='index',columns = ['accuracy','percision','recall','f1_score'])"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.8 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"eb8d3085d426e5e8dee3252ffbadecffc2a8c816985f9d79249a1de42c265c0a"}}},"nbformat":4,"nbformat_minor":2}
